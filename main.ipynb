{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hust512/DQN-DDPG_Stock_Trading/blob/master/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "S7SK50udGidp",
        "colab_type": "code",
        "outputId": "debc6ce3-14fe-42ef-bace-bd23ac213ef5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 847
        }
      },
      "cell_type": "code",
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "\n",
        "\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive\n",
        "\n",
        "import os\n",
        "os.chdir(\"drive/sparse-opt-NN/test-LISTA\")\n",
        "!ls\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 113597 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.1-0ubuntu3~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.1-0ubuntu3~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.1-0ubuntu3~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n",
            "config.py  data  main.ipynb  models  __pycache__  README.md  utils\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nvqbRK5KGAl5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "b6d9c3e6-ee95-4727-8594-fbc6affab5f7"
      },
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "\"\"\"\n",
        "file  : main.py\n",
        "author: Xiaohan Chen\n",
        "email : chernxh@tamu.edu\n",
        "last_modified: 2018-10-13\n",
        "\n",
        "Main script. Start running model from main.py.\n",
        "\"\"\"\n",
        "\n",
        "import os , sys\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # BE QUIET!!!!\n",
        "\n",
        "# timing\n",
        "import time\n",
        "from datetime import timedelta\n",
        "\n",
        "from config import get_config\n",
        "import utils.prob as problem\n",
        "import utils.data as data\n",
        "import utils.train as train\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "try :\n",
        "    from PIL import Image\n",
        "    from sklearn.feature_extraction.image \\\n",
        "            import extract_patches_2d, reconstruct_from_patches_2d\n",
        "except Exception as e :\n",
        "    pass\n",
        "\n",
        "\n",
        "def imread_CS_py(im_fn, patch_size, stride):\n",
        "    im_org = np.array (Image.open (im_fn), dtype='float32')\n",
        "    H, W   = im_org.shape\n",
        "    num_rpatch = (H - patch_size + stride - 1) // stride + 1\n",
        "    num_cpatch = (W - patch_size + stride - 1) // stride + 1\n",
        "    H_pad = patch_size + (num_rpatch - 1) * stride\n",
        "    W_pad = patch_size + (num_cpatch - 1) * stride\n",
        "    im_pad = np.zeros ((H_pad, W_pad), dtype=np.float32)\n",
        "    im_pad [:H, :W] = im_org\n",
        "\n",
        "    return im_org, H, W, im_pad, H_pad, W_pad\n",
        "\n",
        "\n",
        "def img2col_py(im_pad, patch_size, stride):\n",
        "    [H, W] = im_pad.shape\n",
        "    num_rpatch = (H - patch_size) / stride + 1\n",
        "    num_cpatch = (W - patch_size) / stride + 1\n",
        "    num_patches = int (num_rpatch * num_cpatch)\n",
        "    img_col = np.zeros ([patch_size**2, num_patches])\n",
        "    count = 0\n",
        "    for x in range(0, H-patch_size+1, stride):\n",
        "        for y in range(0, W-patch_size+1, stride):\n",
        "            img_col[:, count] = im_pad[x:x+patch_size, y:y+patch_size].reshape([-1])\n",
        "            count = count + 1\n",
        "    return img_col\n",
        "\n",
        "\n",
        "def col2im_CS_py(X_col, patch_size, stride, H, W, H_pad, W_pad):\n",
        "    X0_rec = np.zeros ((H_pad, W_pad))\n",
        "    counts = np.zeros ((H_pad, W_pad))\n",
        "    k = 0\n",
        "    for x in range(0, H_pad-patch_size+1, stride):\n",
        "        for y in range(0, W_pad-patch_size+1, stride):\n",
        "            X0_rec[x:x+patch_size, y:y+patch_size] += X_col[:,k].\\\n",
        "                    reshape([patch_size, patch_size])\n",
        "            counts[x:x+patch_size, y:y+patch_size] += 1\n",
        "            k = k + 1\n",
        "    X0_rec /= counts\n",
        "    X_rec = X0_rec[:H, :W]\n",
        "    return X_rec\n",
        "\n",
        "\n",
        "def setup_model (config , **kwargs) :\n",
        "    untiedf = 'u' if config.untied else 't'\n",
        "    coordf  = 'c' if config.coord  else 's'\n",
        "\n",
        "    \"\"\"LISTA\"\"\"\n",
        "    if config.net == 'LISTA' :\n",
        "        config.model = (\"LISTA_T{T}_lam{lam}_{untiedf}_{coordf}_{exp_id}\"\n",
        "                        .format (T=config.T, lam=config.lam, untiedf=untiedf,\n",
        "                                 coordf=coordf, exp_id=config.exp_id))\n",
        "        from models.LISTA import LISTA\n",
        "        model = LISTA (kwargs ['A'], T=config.T, lam=config.lam,\n",
        "                       untied=config.untied, coord=config.coord,\n",
        "                       scope=config.scope)\n",
        "\n",
        "    \"\"\"LAMP\"\"\"\n",
        "    if config.net == 'LAMP' :\n",
        "        config.model = (\"LAMP_T{T}_lam{lam}_{untiedf}_{coordf}_{exp_id}\"\n",
        "                        .format (T=config.T, lam=config.lam, untiedf=untiedf,\n",
        "                                 coordf=coordf, exp_id=config.exp_id))\n",
        "        from models.LAMP import LAMP\n",
        "        model = LAMP (kwargs ['A'], T=config.T, lam=config.lam,\n",
        "                      untied=config.untied, coord=config.coord,\n",
        "                      scope=config.scope)\n",
        "\n",
        "    \"\"\"LIHT\"\"\"\n",
        "    if config.net == 'LIHT' :\n",
        "        from models.LIHT import LIHT\n",
        "        model = LIHT (p, T=config.T, lam=config.lam, y_=p.y_ , x0_=None ,\n",
        "                      untied=config.untied , cord=config.coord)\n",
        "\n",
        "    \"\"\"LISTA-CP\"\"\"\n",
        "    if config.net == 'LISTA_cp' :\n",
        "        config.model = (\"LISTA_cp_T{T}_lam{lam}_{untiedf}_{coordf}_{exp_id}\"\n",
        "                        .format (T=config.T, lam=config.lam, untiedf=untiedf,\n",
        "                                 coordf=coordf, exp_id=config.exp_id))\n",
        "        from models.LISTA_cp import LISTA_cp\n",
        "        model = LISTA_cp (kwargs ['A'], T=config.T, lam=config.lam,\n",
        "                          untied=config.untied, coord=config.coord,\n",
        "                          scope=config.scope)\n",
        "\n",
        "    \"\"\"LISTA-SS\"\"\"\n",
        "    if config.net == 'LISTA_ss' :\n",
        "        config.model = (\"LISTA_ss_T{T}_lam{lam}_p{p}_mp{mp}_\"\n",
        "                        \"{untiedf}_{coordf}_{exp_id}\"\n",
        "                        .format (T=config.T, lam=config.lam, p=config.percent,\n",
        "                                 mp=config.max_percent, untiedf=untiedf,\n",
        "                                 coordf=coordf, exp_id=config.exp_id))\n",
        "        from models.LISTA_ss import LISTA_ss\n",
        "        model = LISTA_ss (kwargs ['A'], T=config.T, lam=config.lam,\n",
        "                          percent=config.percent, max_percent=config.max_percent,\n",
        "                          untied=config.untied , coord=config.coord,\n",
        "                          scope=config.scope)\n",
        "\n",
        "    \"\"\"LISTA-CPSS\"\"\"\n",
        "    if config.net == 'LISTA_cpss' :\n",
        "        config.model = (\"LISTA_cpss_T{T}_lam{lam}_p{p}_mp{mp}_\"\n",
        "                        \"{untiedf}_{coordf}_{exp_id}\"\n",
        "                        .format (T=config.T, lam=config.lam, p=config.percent,\n",
        "                                 mp=config.max_percent, untiedf=untiedf,\n",
        "                                 coordf=coordf, exp_id=config.exp_id))\n",
        "        from models.LISTA_cpss import LISTA_cpss\n",
        "        model = LISTA_cpss (kwargs ['A'], T=config.T, lam=config.lam,\n",
        "                            percent=config.percent, max_percent=config.max_percent,\n",
        "                            untied=config.untied , coord=config.coord,\n",
        "                            scope=config.scope)\n",
        "\n",
        "    \"\"\"LISTA-CS\"\"\"\n",
        "    if config.net == 'LISTA_cs' :\n",
        "        config.model = (\"LISTA_cs_T{T}_lam{lam}_llam{llam}_\"\n",
        "                        \"{untiedf}_{coordf}_{exp_id}\"\n",
        "                        .format (T=config.T, lam=config.lam, llam=config.lasso_lam,\n",
        "                                 untiedf=untiedf, coordf=coordf,\n",
        "                                 exp_id=config.exp_id))\n",
        "        from models.LISTA_cs import LISTA_cs\n",
        "        model = LISTA_cs (kwargs ['Phi'], kwargs ['D'], T=config.T,\n",
        "                          lam=config.lam, untied=config.untied,\n",
        "                          coord=config.coord, scope=config.scope)\n",
        "\n",
        "    \"\"\"LISTA-SS-CS\"\"\"\n",
        "    if config.net == 'LISTA_ss_cs' :\n",
        "        config.model = (\"LISTA_ss_cs_T{T}_lam{lam}_p{p}_mp{mp}_llam{llam}_\"\n",
        "                        \"{untiedf}_{coordf}_{exp_id}\"\n",
        "                        .format (T=config.T, lam=config.lam, p=config.percent,\n",
        "                                 mp=config.max_percent, llam=config.lasso_lam,\n",
        "                                 untiedf=untiedf, coordf=coordf,\n",
        "                                 exp_id=config.exp_id))\n",
        "        from models.LISTA_ss_cs import LISTA_ss_cs\n",
        "        model = LISTA_ss_cs (kwargs ['Phi'], kwargs ['D'], T=config.T,\n",
        "                             lam=config.lam, percent=config.percent,\n",
        "                             max_percent=config.max_percent,\n",
        "                             untied=config.untied, coord=config.coord,\n",
        "                             scope=config.scope)\n",
        "\n",
        "    \"\"\"LISTA-CPSS-CS\"\"\"\n",
        "    if config.net == 'LISTA_cpss_cs' :\n",
        "        config.model = (\"LISTA_cpss_cs_T{T}_lam{lam}_p{p}_mp{mp}_llam{llam}_\"\n",
        "                        \"{untiedf}_{coordf}_{exp_id}\"\n",
        "                        .format (T=config.T, lam=config.lam, p=config.percent,\n",
        "                                 mp=config.max_percent, llam=config.lasso_lam,\n",
        "                                 untiedf=untiedf, coordf=coordf,\n",
        "                                 exp_id=config.exp_id))\n",
        "        from models.LISTA_cpss_cs import LISTA_cpss_cs\n",
        "        model = LISTA_cpss_cs (kwargs ['Phi'], kwargs ['D'], T=config.T,\n",
        "                               lam=config.lam, percent=config.percent,\n",
        "                               max_percent=config.max_percent,\n",
        "                               untied=config.untied, coord=config.coord,\n",
        "                               scope=config.scope)\n",
        "\n",
        "\n",
        "    config.modelfn = os.path.join (config.expbase, config.model)\n",
        "    config.resfn   = os.path.join (config.resbase, config.model)\n",
        "    print (\"model disc:\", config.model)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "############################################################\n",
        "######################   Training    #######################\n",
        "############################################################\n",
        "\n",
        "def run_train (config) :\n",
        "    if config.task_type == 'sc':\n",
        "        run_sc_train (config)\n",
        "    elif config.task_type == 'cs':\n",
        "        run_cs_train (config)\n",
        "\n",
        "\n",
        "def run_sc_train (config) :\n",
        "    \"\"\"Load problem.\"\"\"\n",
        "    if not os.path.exists (config.probfn):\n",
        "        raise ValueError (\"Problem file not found.\")\n",
        "    else:\n",
        "        p = problem.load_problem (config.probfn)\n",
        "\n",
        "    \"\"\"Set up model.\"\"\"\n",
        "    model = setup_model (config, A=p.A)\n",
        "\n",
        "    \"\"\"Set up input.\"\"\"\n",
        "    config.SNR = np.inf if config.SNR == 'inf' else float (config.SNR)\n",
        "    y_, x_, y_val_, x_val_ = (\n",
        "        train.setup_input_sc (\n",
        "            config.test, p, config.tbs, config.vbs, config.fixval,\n",
        "            config.supp_prob, config.SNR, config.magdist, **config.distargs))\n",
        "\n",
        "    \"\"\"Set up training.\"\"\"\n",
        "    stages = train.setup_sc_training (\n",
        "            model, y_, x_, y_val_, x_val_, None,\n",
        "            config.init_lr, config.decay_rate, config.lr_decay)\n",
        "\n",
        "\n",
        "    tfconfig = tf.ConfigProto (allow_soft_placement=True)\n",
        "    tfconfig.gpu_options.allow_growth = True\n",
        "    with tf.Session (config=tfconfig) as sess:\n",
        "        # graph initialization\n",
        "        sess.run (tf.global_variables_initializer ())\n",
        "\n",
        "        # start timer\n",
        "        start = time.time ()\n",
        "\n",
        "        # train model\n",
        "        model.do_training (sess, stages, config.modelfn, config.scope,\n",
        "                           config.val_step, config.maxit, config.better_wait)\n",
        "\n",
        "        # end timer\n",
        "        end = time.time ()\n",
        "        elapsed = end - start\n",
        "        print (\"elapsed time of training = \" + str (timedelta (seconds=elapsed)))\n",
        "\n",
        "\n",
        "def run_cs_train (config) :\n",
        "    \"\"\"Load dictionary and sensing matrix.\"\"\"\n",
        "    Phi = np.load (config.sensing) ['A']\n",
        "    D   = np.load (config.dict)\n",
        "\n",
        "    \"\"\"Set up model.\"\"\"\n",
        "    model = setup_model (config, Phi=Phi, D=D)\n",
        "\n",
        "    \"\"\"Set up inputs.\"\"\"\n",
        "    y_, f_, y_val_, f_val_ = train.setup_input_cs (config.train_file,\n",
        "                                                   config.val_file,\n",
        "                                                   config.tbs, config.vbs)\n",
        "\n",
        "    \"\"\"Set up training.\"\"\"\n",
        "    stages = train.setup_cs_training (\n",
        "        model, y_, f_, y_val_, f_val_, None, config.init_lr, config.decay_rate,\n",
        "        config.lr_decay, config.lasso_lam)\n",
        "\n",
        "\n",
        "    \"\"\"Start training.\"\"\"\n",
        "    tfconfig = tf.ConfigProto (allow_soft_placement=True)\n",
        "    tfconfig.gpu_options.allow_growth = True\n",
        "    with tf.Session (config=tfconfig) as sess:\n",
        "        # graph initialization\n",
        "        sess.run (tf.global_variables_initializer ())\n",
        "\n",
        "        # start timer\n",
        "        start = time.time ()\n",
        "\n",
        "        # train model\n",
        "        model.do_training (sess, stages, config.modelfn, config.scope,\n",
        "                           config.val_step, config.maxit, config.better_wait)\n",
        "\n",
        "        # end timer\n",
        "        end = time.time ()\n",
        "        elapsed = end - start\n",
        "        print (\"elapsed time of training = \" + str (timedelta (seconds=elapsed)))\n",
        "\n",
        "\n",
        "############################################################\n",
        "######################   Testing    ########################\n",
        "############################################################\n",
        "\n",
        "def run_test (config):\n",
        "    if config.task_type == 'sc':\n",
        "        run_sc_test (config)\n",
        "    elif config.task_type == 'cs':\n",
        "        run_cs_test (config)\n",
        "\n",
        "def run_sc_test (config) :\n",
        "    \"\"\"\n",
        "    Test model.\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"Load problem.\"\"\"\n",
        "    if not os.path.exists (config.probfn):\n",
        "        raise ValueError (\"Problem file not found.\")\n",
        "    else:\n",
        "        p = problem.load_problem (config.probfn)\n",
        "\n",
        "    \"\"\"Load testing data.\"\"\"\n",
        "    xt = np.load (config.xtest)\n",
        "    \"\"\"Set up input for testing.\"\"\"\n",
        "    config.SNR = np.inf if config.SNR == 'inf' else float (config.SNR)\n",
        "    input_, label_ = (\n",
        "        train.setup_input_sc (config.test, p, xt.shape [1], None, False,\n",
        "                              config.supp_prob, config.SNR,\n",
        "                              config.magdist, **config.distargs))\n",
        "\n",
        "    \"\"\"Set up model.\"\"\"\n",
        "    model = setup_model (config , A=p.A)\n",
        "    xhs_ = model.inference (input_, None, False)\n",
        "\n",
        "    \"\"\"Create session and initialize the graph.\"\"\"\n",
        "    tfconfig = tf.ConfigProto (allow_soft_placement=True)\n",
        "    tfconfig.gpu_options.allow_growth = True\n",
        "    with tf.Session (config=tfconfig) as sess:\n",
        "        # graph initialization\n",
        "        sess.run (tf.global_variables_initializer ())\n",
        "        # load model\n",
        "        model.load_trainable_variables (sess , config.modelfn)\n",
        "\n",
        "        nmse_denom = np.sum (np.square (xt))\n",
        "        supp_gt = xt != 0\n",
        "\n",
        "        lnmse  = []\n",
        "        lspar  = []\n",
        "        lsperr = []\n",
        "        lflspo = []\n",
        "        lflsne = []\n",
        "\n",
        "        # test model\n",
        "        for xh_ in xhs_ :\n",
        "            xh = sess.run (xh_ , feed_dict={label_:xt})\n",
        "\n",
        "            # nmse:\n",
        "            loss = np.sum (np.square (xh - xt))\n",
        "            nmse_dB = 10.0 * np.log10 (loss / nmse_denom)\n",
        "            print (nmse_dB)\n",
        "            lnmse.append (nmse_dB)\n",
        "\n",
        "            supp = xh != 0.0\n",
        "            # intermediate sparsity\n",
        "            spar = np.sum (supp , axis=0)\n",
        "            lspar.append (spar)\n",
        "\n",
        "            # support error\n",
        "            sperr = np.logical_xor(supp, supp_gt)\n",
        "            lsperr.append (np.sum (sperr , axis=0))\n",
        "\n",
        "            # false positive\n",
        "            flspo = np.logical_and (supp , np.logical_not (supp_gt))\n",
        "            lflspo.append (np.sum (flspo , axis=0))\n",
        "\n",
        "            # false negative\n",
        "            flsne = np.logical_and (supp_gt , np.logical_not (supp))\n",
        "            lflsne.append (np.sum (flsne , axis=0))\n",
        "\n",
        "    res = dict (nmse=np.asarray  (lnmse),\n",
        "                spar=np.asarray  (lspar),\n",
        "                sperr=np.asarray (lsperr),\n",
        "                flspo=np.asarray (lflspo),\n",
        "                flsne=np.asarray (lflsne))\n",
        "\n",
        "    np.savez (config.resfn , **res)\n",
        "    # end of test\n",
        "\n",
        "\n",
        "def run_cs_test (config) :\n",
        "    \"\"\"Load dictionary and sensing matrix.\"\"\"\n",
        "    Phi = np.load (config.sensing) ['A']\n",
        "    D   = np.load (config.dict)\n",
        "\n",
        "    # loading compressive sensing settings\n",
        "    M = Phi.shape [0]\n",
        "    F = Phi.shape [1]\n",
        "    N = D.shape [1]\n",
        "    assert M == config.M and F == config.F and N == config.N\n",
        "    patch_size = int (np.sqrt (F))\n",
        "    assert patch_size ** 2 == F\n",
        "\n",
        "\n",
        "    \"\"\"Set up model.\"\"\"\n",
        "    model = setup_model (config, Phi=Phi, D=D)\n",
        "\n",
        "    \"\"\"Inference.\"\"\"\n",
        "    y_ = tf.placeholder (shape=(M, None), dtype=tf.float32)\n",
        "    _, fhs_ = model.inference (y_, None)\n",
        "\n",
        "\n",
        "    \"\"\"Start testing.\"\"\"\n",
        "    tfconfig = tf.ConfigProto (allow_soft_placement=True)\n",
        "    tfconfig.gpu_options.allow_growth = True\n",
        "    with tf.Session (config=tfconfig) as sess:\n",
        "\n",
        "        # graph initialization\n",
        "        sess.run (tf.global_variables_initializer ())\n",
        "        # load model\n",
        "        model.load_trainable_variables (sess , config.modelfn)\n",
        "\n",
        "        # calculate average NMSE and PSRN on test images\n",
        "        test_dir = './data/test_images/'\n",
        "        test_files = os.listdir (test_dir)\n",
        "        avg_nmse = 0.0\n",
        "        avg_psnr = 0.0\n",
        "        overlap = 0\n",
        "        stride = patch_size - overlap\n",
        "        if 'joint' in config.net :\n",
        "            D = sess.run (model.D_)\n",
        "        for test_fn in test_files :\n",
        "            # read in image\n",
        "            test_fn = os.path.join (test_dir, test_fn)\n",
        "            test_im, H, W, test_im_pad, H_pad, W_pad = \\\n",
        "                    imread_CS_py (test_fn, patch_size, stride)\n",
        "            test_fs = img2col_py (test_im_pad, patch_size, stride)\n",
        "\n",
        "            # remove dc from features\n",
        "            test_dc = np.mean (test_fs, axis=0, keepdims=True)\n",
        "            test_cfs = test_fs - test_dc\n",
        "            test_cfs = np.asarray (test_cfs) / 255.0\n",
        "\n",
        "            # sensing signals\n",
        "            test_ys = np.matmul (Phi, test_cfs)\n",
        "            num_patch = test_ys.shape [1]\n",
        "\n",
        "            rec_cfs = sess.run (fhs_ [-1], feed_dict={y_: test_ys})\n",
        "            print (rec_cfs.shape)\n",
        "            rec_fs  = rec_cfs * 255.0 + test_dc\n",
        "\n",
        "            # patch-level NMSE\n",
        "            patch_err = np.sum (np.square (rec_fs - test_fs))\n",
        "            patch_denom = np.sum (np.square (test_fs))\n",
        "            avg_nmse += 10.0 * np.log10 (patch_err / patch_denom)\n",
        "\n",
        "            rec_im = col2im_CS_py (rec_fs, patch_size, stride,\n",
        "                                   H, W, H_pad, W_pad)\n",
        "\n",
        "            # image-level PSNR\n",
        "            image_mse = np.mean (np.square (rec_im - test_im))\n",
        "            avg_psnr += 10.0 * np.log10 (255.**2 / image_mse)\n",
        "\n",
        "    num_test_ims = len (test_files)\n",
        "    print ('Average Patch-level NMSE is {}'.format (avg_nmse / num_test_ims))\n",
        "    print ('Average Image-level PSNR is {}'.format (avg_psnr / num_test_ims))\n",
        "\n",
        "    # end of cs_testing\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-28c379d656c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtimedelta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprob\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mproblem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'config'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "GtZO0VFWQzrh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "############################################################\n",
        "#######################    Main    #########################\n",
        "############################################################\n",
        "\n",
        "def main ():\n",
        "    # parse configuration\n",
        "    \n",
        "    config, _ = get_config()\n",
        "#     config, _ = get_config()\n",
        "    # set visible GPUs\n",
        "    os.environ['CUDA_VISIBLE_DEVICES'] = config.gpu\n",
        "\n",
        "    if config.test:\n",
        "        run_test (config)\n",
        "    else:\n",
        "        run_train (config)\n",
        "    # end of main\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main ()\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pwg_zlY0Tjf0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "\"\"\"\n",
        "config.py\n",
        "author: xhchrn\n",
        "        chernxh@tamu.edu\n",
        "\n",
        "Set up experiment configuration using argparse library.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import datetime\n",
        "import argparse\n",
        "\n",
        "def str2bool(v):\n",
        "    return v.lower() in ( 'true' , '1' )\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "\n",
        "# Network arguments\n",
        "net_arg = parser.add_argument_group ('net')\n",
        "net_arg.add_argument (\n",
        "    '-n', '--net', type=str,\n",
        "    help='Network name.')\n",
        "net_arg.add_argument (\n",
        "    '-T', '--T', type=int, default=16,\n",
        "    help=\"Number of layers of LISTA.\")\n",
        "net_arg.add_argument (\n",
        "    '-p', '--percent', type=float, default=0.8,\n",
        "    help=\"Percent of entries to be selected as support in each layer.\")\n",
        "net_arg.add_argument (\n",
        "    '-maxp', '--max_percent', type=float, default=0.0,\n",
        "    help=\"Maximum percentage of entries to be selectedas support in each layer.\")\n",
        "net_arg.add_argument (\n",
        "    '-l', '--lam', type=float, default=0.4,\n",
        "    help=\"Initial lambda in LISTA solvers.\")\n",
        "net_arg.add_argument (\n",
        "    '-u', '--untied', action='store_true',\n",
        "    help=\"Whether weights are untied between layers.\")\n",
        "net_arg.add_argument (\n",
        "    '-c' , '--coord' , action='store_true',\n",
        "    help=\"Whether use independent vector thresholds.\")\n",
        "net_arg.add_argument (\n",
        "    '-sc', '--scope', type=str, default=\"\",\n",
        "    help=\"Scope name of the model.\")\n",
        "\n",
        "# Problem arguments\n",
        "prob_arg = parser.add_argument_group ('prob')\n",
        "prob_arg.add_argument(\n",
        "    '-M', '--M', type=int, default=250,\n",
        "    help=\"Dimension of measurements.\")\n",
        "prob_arg.add_argument(\n",
        "    '-N', '--N', type=int, default=500,\n",
        "    help=\"Dimension of sparse codes.\")\n",
        "prob_arg.add_argument (\n",
        "    '-F', '--F', type=int, default=256,\n",
        "    help='Number of features of extracted patches.')\n",
        "prob_arg.add_argument (\n",
        "    '-sr', '--sample_rate', type=int, default=50,\n",
        "    help=\"Sampling rate in compressive sensing experiments.\")\n",
        "prob_arg.add_argument(\n",
        "    '-P', '--pnz', type=float, default=0.1,\n",
        "    help=\"Percent of nonzero entries in sparse codes.\")\n",
        "prob_arg.add_argument(\n",
        "    '-S', '--SNR', type=str, default='inf',\n",
        "    help=\"Strength of noises.\")\n",
        "prob_arg.add_argument(\n",
        "    '-C', '--con_num', type=float, default=0.0,\n",
        "    help=\"Condition number of measurement matrix.\")\n",
        "prob_arg.add_argument(\n",
        "    '-CN', '--col_normalized', type=str2bool, default=True,\n",
        "    help=\"Flag of whether normalize the columns of the dictionary or sensing matrix.\")\n",
        "prob_arg.add_argument (\n",
        "    '-task', '--task_type', type=str, default='sc',\n",
        "    help='Task type, in [`sc`, `cs`].')\n",
        "prob_arg.add_argument (\n",
        "    '-llam', '--lasso_lam', type=float, default=0.2,\n",
        "    help='The weight of l1 norm term `labmda` in LASSO.')\n",
        "\n",
        "\"\"\"Training arguments.\"\"\"\n",
        "train_arg = parser.add_argument_group ('train')\n",
        "train_arg.add_argument (\n",
        "    '-lr', '--init_lr', type=float, default=5e-4,\n",
        "    help=\"Initial learning rate.\")\n",
        "train_arg.add_argument (\n",
        "    '-tbs', '--tbs', type=int, default=64,\n",
        "    help=\"Training batch size.\")\n",
        "train_arg.add_argument (\n",
        "    '-vbs', '--vbs', type=int, default=1000,\n",
        "    help=\"Validation batch size.\")\n",
        "train_arg.add_argument (\n",
        "    '-fixval', '--fixval', type=str2bool, default=True,\n",
        "    help=\"Flag of whether we fix a validation set.\")\n",
        "train_arg.add_argument (\n",
        "    '-supp_prob', '--supp_prob', type=str, default=None,\n",
        "    help=\"The probability distribution of support we use in trianing.\")\n",
        "train_arg.add_argument (\n",
        "    '-magdist', '--magdist', type=str, default='normal',\n",
        "    help=\"Type of the magnitude distribution.\")\n",
        "train_arg.add_argument (\n",
        "    '-nmean', '--magnmean', type=float, default=0.0,\n",
        "    help=\"The expectation of Gaussian that we use to sample magnitudes.\")\n",
        "train_arg.add_argument (\n",
        "    '-nstd', '--magnstd', type=float, default=1.0,\n",
        "    help=\"The standard deviation of Gaussain we use to sample magnitudes.\")\n",
        "train_arg.add_argument (\n",
        "    '-bp', '--magbp', type=float, default=0.5,\n",
        "    help=\"The probability that the magnitudes take value `magbv0` when they are\"\n",
        "         \"sampled from Bernoulli.\")\n",
        "train_arg.add_argument (\n",
        "    '-bv0', '--magbv0', type=float, default=1.0,\n",
        "    help=\"The value that the magnitudes take with probability `magbp` when they\"\n",
        "         \"are sampled from Bernoulli.\")\n",
        "train_arg.add_argument (\n",
        "    '-bv1', '--magbv1', type=float, default=1.0,\n",
        "    help=\"The value that the magnitudes take with probability `1-magbp` when\"\n",
        "         \"they are sampled from Bernoulli.\")\n",
        "train_arg.add_argument (\n",
        "    '-dr', '--decay_rate', type=float, default=0.3,\n",
        "    help=\"Learning rate decaying rate after training each layer.\")\n",
        "train_arg.add_argument (\n",
        "    '-ld', '--lr_decay', type=str, default='0.2,0.02',\n",
        "    help=\"Learning rate decaying rate after training each layer.\")\n",
        "train_arg.add_argument (\n",
        "    '-vs', '--val_step', type=int, default=10,\n",
        "    help=\"Interval of validation in training.\")\n",
        "train_arg.add_argument (\n",
        "    '-mi', '--maxit', type=int, default=200000,\n",
        "    help=\"Max number iteration of each stage.\")\n",
        "train_arg.add_argument (\n",
        "    '-bw', '--better_wait', type=int, default=4000,\n",
        "    help=\"Waiting time before jumping to next stage.\")\n",
        "\n",
        "\n",
        "# Experiments arguments\n",
        "exp_arg = parser.add_argument_group ('exp')\n",
        "exp_arg.add_argument (\n",
        "    '-id', '--exp_id', type=int, default=0,\n",
        "    help=\"ID of the experiment/model.\")\n",
        "exp_arg.add_argument (\n",
        "    '-ef', '--exp_folder', type=str, default='./experiments',\n",
        "    help=\"Root folder for problems and momdels.\")\n",
        "exp_arg.add_argument (\n",
        "    '-rf', '--res_folder', type=str, default='./results',\n",
        "    help=\"Root folder where test results are saved.\")\n",
        "exp_arg.add_argument (\n",
        "    '-pf', '--prob_folder', type=str, default='',\n",
        "    help=\"Subfolder in exp_folder for a specific setting of problem.\")\n",
        "exp_arg.add_argument (\n",
        "    '--prob', type=str, default='prob.npz',\n",
        "    help=\"Problem file name in prob_folder.\")\n",
        "exp_arg.add_argument (\n",
        "    '-se', '--sensing', type=str, default=None,\n",
        "    help=\"Sensing matrix file. Instance of Problem class.\")\n",
        "exp_arg.add_argument (\n",
        "    '-dc', '--dict', type=str, default=None,\n",
        "    help=\"Dictionary file. Numpy array instance stored as npy file.\")\n",
        "exp_arg.add_argument (\n",
        "    '-df', '--data_folder', type=str, default=None,\n",
        "    help=\"Folder where the tfrecords datasets are stored.\")\n",
        "exp_arg.add_argument (\n",
        "    '-tf', '--train_file', type=str, default='train.tfrecords',\n",
        "    help=\"File name of tfrecords file of training data for cs exps.\")\n",
        "exp_arg.add_argument (\n",
        "    '-vf', '--val_file', type=str, default='val.tfrecords',\n",
        "    help=\"File name of tfrecords file of validation data for cs exps.\")\n",
        "exp_arg.add_argument (\n",
        "    '-col', '--column', type=str2bool, default=False,\n",
        "    help=\"Flag of whether column-based model is used.\")\n",
        "exp_arg.add_argument (\n",
        "    '-t' , '--test' , action='store_true' ,\n",
        "    help=\"Flag of training or testing models.\")\n",
        "exp_arg.add_argument (\n",
        "    '-np', '--norm_patch', type=str2bool, default=False,\n",
        "    help=\"Flag of normalizing patches in training and testing.\")\n",
        "exp_arg.add_argument (\n",
        "    '-xt', '--xtest', type=str, default='./data/xtest_n500_p10.npy',\n",
        "    help='Default test x input for simulation experiments.')\n",
        "exp_arg.add_argument (\n",
        "    '-g', '--gpu', type=str, default='0',\n",
        "    help=\"ID's of allocated GPUs.\")\n",
        "\n",
        "\n",
        "def get_config():\n",
        "    config, unparsed = parser.parse_known_args ()\n",
        "\n",
        "    \"\"\"\n",
        "    Check validity of arguments.\n",
        "    \"\"\"\n",
        "    # check if a network model is specified\n",
        "    if config.net == None:\n",
        "        raise ValueError ( 'no model specified' )\n",
        "\n",
        "    # set experiment path and folder\n",
        "    if not os.path.exists ( config.exp_folder ):\n",
        "        os.mkdir ( config.exp_folder )\n",
        "\n",
        "\n",
        "    \"\"\"Experiments and results base folder.\"\"\"\n",
        "    if config.task_type == 'sc':\n",
        "        config.prob_folder = ('m{}_n{}_k{}_p{}_s{}'.format (\n",
        "                                config.M , config.N , config.con_num ,\n",
        "                                config.pnz , config.SNR ))\n",
        "    elif config.task_type == 'cs':\n",
        "        # check problem folder: dictionary and sensing matrix\n",
        "        config.prob_folder = ('cs_bsd_d{}-{}'.format (config.F, config.N))\n",
        "\n",
        "    # make experiment base path and results base path\n",
        "    setattr (config , 'expbase' , os.path.join (config.exp_folder,\n",
        "                                                config.prob_folder ) )\n",
        "    setattr (config , 'resbase' , os.path.join (config.res_folder,\n",
        "                                                config.prob_folder))\n",
        "    if not os.path.exists (config.expbase):\n",
        "        os.mkdir (config.expbase)\n",
        "    if not os.path.exists (config.resbase):\n",
        "        os.mkdir (config.resbase)\n",
        "\n",
        "    if config.task_type == 'cs':\n",
        "        config.expbase = os.path.join (config.expbase,\n",
        "                                       \"r%d\" % config.sample_rate)\n",
        "        config.resbase = os.path.join (config.resbase,\n",
        "                                       \"r%d\" % config.sample_rate)\n",
        "\n",
        "    \"\"\"\n",
        "    Problem file for sparse coding task.\n",
        "    Data folder, dictionary and sensing file for compressive sensing task.\n",
        "    \"\"\"\n",
        "    if config.task_type == 'sc':\n",
        "        setattr (config , 'probfn'  , os.path.join (config.expbase , config.prob))\n",
        "    # Data folder, dictionary and sensing matrix location for cs experiments.\n",
        "    elif config.task_type == 'cs':\n",
        "        # check data files, dictionary and sensing matrix\n",
        "        if config.train_file is None:\n",
        "            raise ValueError (\"Please provide a training tfrecords file for CS exp!\")\n",
        "        if config.val_file is None:\n",
        "            raise ValueError (\"Please provide a validation tfrecords file for CS exp!\")\n",
        "        if config.dict is None:\n",
        "            raise ValueError (\"Please provide a dictionary for CS exp!\")\n",
        "        if config.sensing is None:\n",
        "            raise ValueError (\"Please provide a sensing matrix for CS exp!\")\n",
        "\n",
        "        if not os.path.exists (config.train_file) :\n",
        "            raise ValueError ('No training data tfrecords file found.')\n",
        "        if not os.path.exists (config.val_file) :\n",
        "            raise ValueError ('No validation data tfrecords file found')\n",
        "        if not os.path.exists (config.sensing) :\n",
        "            raise ValueError ('No sensing matrix file found')\n",
        "        if not os.path.exists (config.dict) :\n",
        "            raise ValueError ('No dictionary matrix file found')\n",
        "\n",
        "\n",
        "    # lr_decay\n",
        "    config.lr_decay = tuple ([float(decay) for decay in config.lr_decay.split (',')])\n",
        "\n",
        "\n",
        "    \"\"\"Support and magnitudes distribution settings for sparse coding task.\"\"\"\n",
        "    if config.task_type == 'sc':\n",
        "        # supp_prob\n",
        "        if not config.supp_prob is None:\n",
        "            try:\n",
        "                config.supp_prob = float (config.supp_prob)\n",
        "            except ValueError:\n",
        "                config.supp_prob = np.load (config.supp_prob)\n",
        "\n",
        "        \"\"\"Magnitudes distribution of sparse codes.\"\"\"\n",
        "        if config.magdist == 'normal':\n",
        "            config.distargs = dict (mean=config.magnmean,\n",
        "                                    std=config.magnstd)\n",
        "        elif config.magdist == 'bernoulli':\n",
        "            config.distargs = dict (p=config.magbp,\n",
        "                                    v0=config.magbv0,\n",
        "                                    v1=config.magbv1)\n",
        "\n",
        "    return config, unparsed\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PnaG0cviXLTA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "\"\"\"\n",
        "file  : LISTA.py\n",
        "author: Xiaohan Chen\n",
        "email : chernxh@tamu.edu\n",
        "last_modified : 2018-10-21\n",
        "\n",
        "Implementation of Learned ISTA proposed by LeCun et al in 2010.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import utils.train\n",
        "\n",
        "from utils.tf import shrink\n",
        "from models.LISTA_base import LISTA_base\n",
        "\n",
        "class LISTA (LISTA_base):\n",
        "\n",
        "    \"\"\"\n",
        "    Implementation of LISTA model proposed by LeCun in 2010.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, A, T, lam, untied, coord, scope):\n",
        "        \"\"\"\n",
        "        :A      : Numpy ndarray. Dictionary/Sensing matrix.\n",
        "        :T      : Integer. Number of layers (depth) of this LISTA model.\n",
        "        :lam    : Float. The initial weight of l1 loss term in LASSO.\n",
        "        :untied : Boolean. Flag of whether weights are shared within layers.\n",
        "        :scope  : String. Scope name of the model.\n",
        "        \"\"\"\n",
        "        self._A   = A.astype (np.float32)\n",
        "        self._T   = T\n",
        "        self._lam = lam\n",
        "        self._M   = self._A.shape [0]\n",
        "        self._N   = self._A.shape [1]\n",
        "\n",
        "        self._scale = 1.001 * np.linalg.norm (A, ord=2)**2\n",
        "        self._theta = (self._lam / self._scale).astype(np.float32)\n",
        "        if coord:\n",
        "            self._theta = np.ones ((self._N, 1), dtype=np.float32) * self._theta\n",
        "\n",
        "        self._untied = untied\n",
        "        self._coord  = coord\n",
        "        self._scope  = scope\n",
        "\n",
        "        \"\"\" Set up layers.\"\"\"\n",
        "        self.setup_layers()\n",
        "\n",
        "\n",
        "    def setup_layers(self):\n",
        "        \"\"\"\n",
        "        Implementation of LISTA model proposed by LeCun in 2010.\n",
        "\n",
        "        :prob: Problem setting.\n",
        "        :T: Number of layers in LISTA.\n",
        "        :returns:\n",
        "            :layers: List of tuples ( name, xh_, var_list )\n",
        "                :name: description of layers.\n",
        "                :xh: estimation of sparse code at current layer.\n",
        "                :var_list: list of variables to be trained seperately.\n",
        "\n",
        "        \"\"\"\n",
        "        Bs_     = []\n",
        "        Ws_     = []\n",
        "        thetas_ = []\n",
        "\n",
        "        B = (np.transpose (self._A) / self._scale).astype (np.float32)\n",
        "        W = np.eye (self._N, dtype=np.float32) - np.matmul (B, self._A)\n",
        "\n",
        "        with tf.variable_scope (self._scope, reuse=False) as vs:\n",
        "            # constant\n",
        "            self._kA_ = tf.constant (value=self._A, dtype=tf.float32)\n",
        "\n",
        "            Bs_.append (tf.get_variable (name='B', dtype=tf.float32,\n",
        "                                         initializer=B))\n",
        "            Bs_ = Bs_ * self._T\n",
        "            if not self._untied: # tied model\n",
        "                Ws_.append (tf.get_variable (name='W', dtype=tf.float32,\n",
        "                                             initializer=W))\n",
        "                Ws_ = Ws_ * self._T\n",
        "\n",
        "            for t in range (self._T):\n",
        "                thetas_.append (tf.get_variable (name=\"theta_%d\"%(t+1),\n",
        "                                                 dtype=tf.float32,\n",
        "                                                 initializer=self._theta))\n",
        "                if self._untied: # untied model\n",
        "                    Ws_.append (tf.get_variable (name=\"W_%d\"%(t+1),\n",
        "                                                 dtype=tf.float32,\n",
        "                                                 initializer=W))\n",
        "\n",
        "        # Collection of all trainable variables in the model layer by layer.\n",
        "        # We name it as `vars_in_layer` because we will use it in the manner:\n",
        "        # vars_in_layer [t]\n",
        "        self.vars_in_layer = list (zip (Bs_, Ws_, thetas_))\n",
        "\n",
        "    def inference (self, y_, x0_=None):\n",
        "        xhs_  = [] # collection of the regressed sparse codes\n",
        "\n",
        "        if x0_ is None:\n",
        "            batch_size = tf.shape (y_) [-1]\n",
        "            xh_ = tf.zeros (shape=(self._N, batch_size), dtype=tf.float32)\n",
        "        else:\n",
        "            xh_ = x0_\n",
        "        xhs_.append (xh_)\n",
        "\n",
        "        with tf.variable_scope (self._scope, reuse=True) as vs:\n",
        "            for t in range (self._T):\n",
        "                B_, W_, theta_ = self.vars_in_layer [t]\n",
        "\n",
        "                By_ = tf.matmul (B_, y_)\n",
        "                xh_ = shrink (tf.matmul (W_, xh_) + By_, theta_)\n",
        "                xhs_.append (xh_)\n",
        "\n",
        "        return xhs_\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}